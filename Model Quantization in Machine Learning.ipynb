{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "098a4a84",
   "metadata": {},
   "source": [
    "# Model Quantization in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75799a1",
   "metadata": {},
   "source": [
    "Model quantization is a technique used to reduce the memory and computational requirements of machine learning models. It involves converting high-precision model parameters to lower precision, resulting in smaller model sizes and faster inference times. In this example, I'll demonstrate how to use model quantization on a simple machine learning model and compare it with the model without quantization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fe1664",
   "metadata": {},
   "source": [
    "## Model Without Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20e35ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model without quantization: 0.855\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = lr_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of the model without quantization:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13837a5",
   "metadata": {},
   "source": [
    "## Model With Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "598305fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the quantized model: 0.855\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "\n",
    "# Function for quantization\n",
    "def quantize_model(model, quantization_bits=8):\n",
    "    model_weights = []\n",
    "    for layer in model.coef_:\n",
    "        # Scale the weights based on quantization_bits\n",
    "        scale = 2 ** quantization_bits\n",
    "        quantized_layer = np.round(layer * scale) / scale\n",
    "        model_weights.append(quantized_layer)\n",
    "    return model_weights\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Quantize the model\n",
    "quantization_bits = 8  # Adjust as needed\n",
    "quantized_weights = quantize_model(lr_model, quantization_bits)\n",
    "\n",
    "# Save the quantized model to a file\n",
    "with open(\"quantized_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(quantized_weights, f)\n",
    "\n",
    "# Load the quantized model\n",
    "with open(\"quantized_model.pkl\", \"rb\") as f:\n",
    "    quantized_weights = pickle.load(f)\n",
    "\n",
    "# Set the quantized weights to the model\n",
    "lr_model.coef_ = np.array(quantized_weights)\n",
    "\n",
    "# Evaluate the quantized model\n",
    "y_pred_quantized = lr_model.predict(X_test)\n",
    "accuracy_quantized = accuracy_score(y_test, y_pred_quantized)\n",
    "print(\"Accuracy of the quantized model:\", accuracy_quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3bf454",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages of Model Quantization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b7ec57",
   "metadata": {},
   "source": [
    "In the example above, I quantize the weights of the logistic regression model using a specified number of quantization bits. We then save the quantized weights to a file and load them back to apply them to the model. Finally, we evaluate the quantized model and compare its accuracy with the model without quantization. Adjust the quantization_bits value for different levels of quantization and observe the trade-off between model size and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d701d87",
   "metadata": {},
   "source": [
    "Model quantization offers several benefits, especially in the context of machine learning efficiency:\n",
    "\n",
    "- Reduced Memory Footprint:\n",
    "Quantizing the model reduces the precision of the model's parameters, significantly reducing the memory required to store the model. This is crucial for deploying models on memory-constrained devices, such as mobile devices or edge devices.\n",
    "\n",
    "- Faster Inference:\n",
    "Lower precision operations (e.g., int8 instead of float32) are faster to compute on modern hardware, resulting in faster inference times. This is particularly important for real-time applications where low latency is a requirement.\n",
    "\n",
    "- Improved Energy Efficiency:\n",
    "The reduced memory access and faster computation contribute to lower energy consumption during inference, making the model more efficient and cost-effective to run, especially on battery-powered devices.\n",
    "\n",
    "- Scalability:\n",
    "With smaller model sizes, it becomes easier to distribute models over a network, enabling quicker downloads and sharing. This is beneficial for edge computing and federated learning scenarios.\n",
    "\n",
    "- Improved Deployment on Edge Devices:\n",
    "Many edge devices have limited computational resources. Quantization allows machine learning models to run on these devices, enabling on-device processing without relying heavily on cloud-based computation.\n",
    "\n",
    "- Cost-Effective Deployment:\n",
    "By reducing the computational resources required for inference, quantization can lead to cost savings in cloud-based deployments where computation is billed based on usage.\n",
    "\n",
    "- Compatibility:\n",
    "Quantized models are often compatible with a broader range of hardware and software platforms, making them more versatile and easier to integrate into different systems.\n",
    "\n",
    "It's important to note that while quantization provides significant benefits in terms of efficiency, there might be a trade-off in model accuracy due to the loss of precision. However, in many cases, careful tuning of the quantization process can mitigate this loss and still result in an acceptable level of accuracy for the target application. The balance between efficiency gains and accuracy is a critical consideration when applying model quantization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
